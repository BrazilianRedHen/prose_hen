{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTING LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np #Biblioteca de suporte de arrays e matriz multidimensionais\n",
    "import pandas as pd #Biblioteca de manipulação e análise de dado\n",
    "import os #Esse modulo prove alguns metodos para atuar com o sistema operacional, nesse projeto utilizado para gerenciamento de diretorios\n",
    "import sys #Esse modulo prove acesso à algumas variaveis de sistema, nesse projeto utilizado para gerenciamento de diretorios\n",
    "import matplotlib.pyplot as plt #Biblioteca padrão pala plotagem de graficos\n",
    "#import seaborn as sns #Biblioteca para melhoria de interface dos graficos\n",
    "sys.path.append('../abstract_normalizer')\n",
    "import dictionaries \n",
    "import json\n",
    "#import operator\n",
    "from matplotlib.ticker import (MultipleLocator, FormatStrFormatter, AutoMinorLocator)\n",
    "import math\n",
    "import inflection\n",
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DECLARING BASIC FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that removes files that have especified extensions from a list of files 'li'\n",
    "def cleanDirectory(li):\n",
    "    try:\n",
    "        for directory in li:\n",
    "            if(not directory.endswith(\".json\") ):\n",
    "                li.remove(directory)\n",
    "                cleanDirectory(li)\n",
    "    except:\n",
    "        print(\"\")\n",
    "\n",
    "#Function that gets all the fields, disciplines and journalsof the corpus\n",
    "def getAllFields():\n",
    "    return {\n",
    "    \"AGRONOMY\", \n",
    "    \"BUSINESS\", \n",
    "    \"CHEMISTRY\", \n",
    "    \"COGNITIVE LINGUISTICS\",\n",
    "    \"COMPUTER SCIENCE\",\n",
    "    \"ENGINEERING\", \n",
    "    \"HEALTH CARE SCIENCES & SERVICES\", \n",
    "    \"LINGUISTICS\", \n",
    "    \"PHYSICS\", \n",
    "    \"TELECOMMUNICATIONS\"\n",
    "    }\n",
    "\n",
    "def getAllDisciplines():\n",
    "    return {\n",
    "            \"BIOLOGICAL SCIENCES\",\n",
    "            \"SOCIAL & HUMAN SCIENCES\",\n",
    "            \"HARD SCIENCES\"\n",
    "    }\n",
    "\n",
    "def getAllJournals():\n",
    "    return {\n",
    "    \"ACADEMIC MEDICINE\",\n",
    "    \"ACADEMY OF MANAGEMENT ANNALS\" ,\n",
    "    \"ACADEMY OF MANAGEMENT REVIEW\" ,\n",
    "    \"ADVANCED MATERIALS\" ,\n",
    "    \"ADVANCES IN AGRONOMY, VOL 129\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 130\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 131\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 132\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 133\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 134\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 135\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 136\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 137\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 138\",\n",
    "    \"ADVANCES IN AGRONOMY, VOL 139\",\n",
    "    \"AGRICULTURAL AND FOREST METEOROLOGY\",\n",
    "    \"AGRONOMY FOR SUSTAINABLE DEVELOPMENT\",\n",
    "    \"ANNUAL REVIEW OF APPLIED LINGUISTICS\",\n",
    "    \"APPLIED LINGUISTICS\",\n",
    "    \"ARCHIVES OF COMPUTATIONAL METHODS IN ENGINEERING\",\n",
    "    \"BMJ QUALITY & SAFETY\",\n",
    "    \"BRAIN AND LANGUAGE\",\n",
    "    \"CHEMICAL REVIEWS\" ,\n",
    "    \"CHEMICAL SOCIETY REVIEWS\" ,\n",
    "    \"COGNITIVE LINGUISTICS\" ,\n",
    "    \"COMBUSTION AND FLAME\",\n",
    "    \"COMPOSITES PART B ENGINEERING\",\n",
    "    \"COMPUTER AIDED CIVIL AND INFRASTRUCTURE ENGINEERING\",\n",
    "    \"COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING\",\n",
    "    \"ENERGY & ENVIRONMENTAL SCIENCE\" ,\n",
    "    \"GLOBAL CHANGE BIOLOGY BIOENERGY\",\n",
    "    \"HEALTH AFFAIRS\",\n",
    "    \"IEEE COMMUNICATIONS MAGAZINE\",\n",
    "    \"IEEE COMMUNICATIONS SURVEYS AND TUTORIALS\",\n",
    "    \"IEEE JOURNAL ON SELECTED AREAS IN COMMUNICATIONS\",\n",
    "    \"IEEE NETWORK\",\n",
    "    \"IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS\",\n",
    "    \"IEEE TRANSACTIONS ON MEDICAL IMAGING\",\n",
    "    \"IEEE WIRELESS COMMUNICATIONS\",\n",
    "    \"INTERNATIONAL JOURNAL OF ENGINEERING SCIENCE\",\n",
    "    \"JOURNAL OF MANAGEMENT\" ,\n",
    "    \"JOURNAL OF MARKETING\" ,\n",
    "    \"JOURNAL OF MEMORY AND LANGUAGE\",\n",
    "    \"JOURNAL OF SECOND LANGUAGE WRITING\",\n",
    "    \"JOURNAL OF STATISTICAL SOFTWARE\",\n",
    "    \"JOURNAL OF THE ACADEMY OF MARKETING SCIENCE\" ,\n",
    "    \"MEDICAL IMAGE ANALYSIS\",\n",
    "    \"MILBANK QUARTERLY\",\n",
    "    \"MODERN LANGUAGE JOURNAL\",\n",
    "    \"NATURE CHEMISTRY\" ,\n",
    "    \"NATURE PHYSICS\" ,\n",
    "    \"PHYSICAL REVIEW X\" ,\n",
    "    \"PHYSICS REPORTS REVIEW SECTION OF PHYSICS LETTERS\" ,\n",
    "    \"QUALITY & SAFETY\" ,\n",
    "    \"REPORTS ON PROGRESS IN PHYSICS\" ,\n",
    "    \"REVIEW OF COGNITIVE LINGUISTICS\" ,\n",
    "    \"REVIEWS OF MODERN PHYSICS\" ,\n",
    "    \"THEORETICAL AND APPLIED GENETICS\",\n",
    "    \"VALUE IN HEALTH\"\n",
    "    }\n",
    "\n",
    "#Function that gets the Title of the abstract\n",
    "def getTitle(title):\n",
    "    txtFiles = list()\n",
    "    for (dirpath, dirnames, filenames) in os.walk(\"../raw\"):\n",
    "        txtFiles += [os.path.join(dirpath, file) for file in filenames]\n",
    "\n",
    "    for txtFile in txtFiles:\n",
    "        if txtFile.endswith(\".txt\"):\n",
    "            for line in list(open(txtFile)):\n",
    "                if(line.startswith(\"TI \")):\n",
    "                    nextline = list(open(txtFile))[list(open(txtFile)).index(line)+1];\n",
    "                    if(not nextline.startswith(\"SO \")):\n",
    "                        fileTitle = line.replace(\"TI \",\"\").replace(\"\\n\",\" \").replace(\"  \",\" \")+\" \".join(nextline.split())\n",
    "                    else:\n",
    "                        fileTitle = line.replace(\"TI \",\"\").replace(\"\\n\",\" \")\n",
    "                    if(fileTitle == title):\n",
    "                        return fileTitle\n",
    "\n",
    "#Functions that get the Journal based on a file name\n",
    "def searchJournal(file1,file2):\n",
    "    retorno = \"\"\n",
    "    for j in getAllJournals():\n",
    "        if(file1.find(j) != -1):\n",
    "            retorno = file1\n",
    "        elif(file2.find(j) != -1):\n",
    "            retorno = file2\n",
    "        elif( (file2.find(j) != -1) and (file1.find(j) != -1) ):\n",
    "            retorno = \"False\"\n",
    "    return retorno\n",
    "\n",
    "def getJournal(file):\n",
    "    file = file.split('.seg.json')[0]\n",
    "    file1 = file.split('_')[-3].replace('-',' ').upper()\n",
    "    file2 = file.split('_')[-2].replace('-',' ').upper()\n",
    "    journal = searchJournal(file1,file2)\n",
    "    return journal\n",
    "\n",
    "\n",
    "#Function that gets Abstract's info (journal, field, discipline) from a json file 'file'\n",
    "def getInfoFromFile(file):\n",
    "            fields = dictionaries.dictionaryFields()\n",
    "            disciplines = dictionaries.dictionaryDisciplines()\n",
    "            journals = getAllJournals() \n",
    "            journal = \"\"\n",
    "            \n",
    "            journal = getJournal(file)    \n",
    "            \n",
    "            field = fields.get(journal)\n",
    "            discipline = disciplines.get(field)\n",
    "            abstract = dict()\n",
    "            abstract['journal'] = journal\n",
    "            abstract['field']   = field\n",
    "            abstract['discipline'] = discipline\n",
    "            return abstract\n",
    "\n",
    "\n",
    "#Function that orders a dict with index:values by values\n",
    "def sortDict(d):\n",
    "    lista  = (sorted(d.items(), key = lambda x : x[1], reverse=True))\n",
    "    retorno = dict()\n",
    "    for value in lista:\n",
    "        retorno[value[0]] = value[1]\n",
    "    return retorno\n",
    "\n",
    "#Function that gets the interval between the value start and the value end\n",
    "def cutdict(pdict, start, length):\n",
    "    counter = 0\n",
    "    outdict = dict() \n",
    "    pdict = pdict.copy()\n",
    "    labels = list(pdict.keys())\n",
    "    for label in labels:\n",
    "        if(counter>=start and counter < start+length):\n",
    "            outdict[label] = pdict.get(label)\n",
    "        counter += 1\n",
    "    return outdict\n",
    "\n",
    "\n",
    "#Function that creates the directories where charts will be saved\n",
    "def createDirectories():\n",
    "    if not os.path.exists('export'):\n",
    "        os.mkdir('export')\n",
    "\n",
    "    if not os.path.exists('export/field'):\n",
    "        os.mkdir('export/field')\n",
    "\n",
    "    if not os.path.exists('export/discipline'):\n",
    "        os.mkdir('export/discipline')\n",
    "\n",
    "    if not os.path.exists('export/journal'):\n",
    "        os.mkdir('export/journal')\n",
    "\n",
    "    if not os.path.exists('export/all'):\n",
    "        os.mkdir('export/all')\n",
    "        \n",
    "    if not os.path.exists('export/field/frame'):\n",
    "        os.mkdir('export/field/frame')\n",
    "\n",
    "    if not os.path.exists('export/discipline/frame'):\n",
    "        os.mkdir('export/discipline/frame')\n",
    "\n",
    "    if not os.path.exists('export/journal/frame'):\n",
    "        os.mkdir('export/journal/frame')\n",
    "\n",
    "    if not os.path.exists('export/all/frame'):\n",
    "        os.mkdir('export/all/frame')\n",
    "\n",
    "    if not os.path.exists('export/field/lexical_unit'):\n",
    "        os.mkdir('export/field/lexical_unit')\n",
    "\n",
    "    if not os.path.exists('export/discipline/lexical_unit'):\n",
    "        os.mkdir('export/discipline/lexical_unit')\n",
    "\n",
    "    if not os.path.exists('export/journal/lexical_unit'):\n",
    "        os.mkdir('export/journal/lexical_unit')\n",
    "\n",
    "    if not os.path.exists('export/all/lexical_unit'):\n",
    "        os.mkdir('export/all/lexical_unit')\n",
    "        \n",
    "# Function that plots charts\n",
    "def generateChart(d, opt, info, optdir):\n",
    "    \n",
    "    dic = d\n",
    "    \n",
    "    #selecting only the 30 frames with more occurrence\n",
    "    #dic = first30(dic)\n",
    "    \n",
    "    #setting the data that will appear in the chart\n",
    "    labels = list(dic.keys())\n",
    "    values = list(dic.values())\n",
    "    index = np.arange(len(labels))\n",
    "    \n",
    "    #used to allow xaxis locators settings\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    #plotting bar chart\n",
    "    ax.barh(index, values)\n",
    "    \n",
    "    #setting labels\n",
    "    if(optdir == 'frame'):\n",
    "        plt.ylabel('Frames', fontsize=5)\n",
    "    elif(optdir == 'lexical_unit'):\n",
    "        plt.ylabel('Lexical Units', fontsize=5)\n",
    "\n",
    "    plt.xlabel('Occurrences', fontsize=5)\n",
    "    \n",
    "    plt.yticks(index, labels, fontsize=5, rotation=0)\n",
    "    \n",
    "    major = 10\n",
    "    minor = 1\n",
    "    bigger = max(values)\n",
    "    if(bigger > 10):\n",
    "        major = bigger/10\n",
    "        major = math.ceil(major)\n",
    "        while(major % 10 != 0):\n",
    "            major = major+1\n",
    "        minor = major/10\n",
    "    else:\n",
    "        major = 2\n",
    "        minor = 1\n",
    "    \n",
    "    #configuring xtick Major and minor locators\n",
    "    mlocatormax = MultipleLocator(major)\n",
    "    mlocatormin = MultipleLocator(minor)\n",
    "    \n",
    "    ax.xaxis.set_major_locator(mlocatormax)\n",
    "    ax.xaxis.set_major_formatter(FormatStrFormatter('%d'))\n",
    "    ax.xaxis.set_minor_locator(mlocatormin)\n",
    "    ax.xaxis.set_view_interval(0,bigger+major)\n",
    "    \n",
    "    #setting locators size\n",
    "    ax.tick_params(axis='x', which='major', labelsize=6)\n",
    "    ax.tick_params(axis='y', which='major', labelsize=5)\n",
    "    \n",
    "    if(info == 'all'):\n",
    "        info = 'All Corpus'\n",
    "        \n",
    "    plt.title(info)\n",
    "    \n",
    "    if(optdir == 'frame'):\n",
    "        plt.savefig('export/'+opt+'/frame/'+info+'.png', bbox_inches='tight', dpi=300)\n",
    "    elif(optdir == 'lexical_unit'):\n",
    "        plt.savefig('export/'+opt+'/lexical_unit/'+info+'.png', bbox_inches='tight', dpi=300)\n",
    "        \n",
    "    #plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# function that counts the ocurrence of itens from a item_type in abstracts that match a category from a category_type (field,\n",
    "# discipline, journal), adding this to a existing counting dict (itens_counted)\n",
    "def countItens(item_type, itens_counted, *params):\n",
    "    if params: \n",
    "        category = params[0]\n",
    "        category_type = params[1]\n",
    "    #traversing all files\n",
    "    for file_name in files:            \n",
    "        file_location = path+'/'+file_name\n",
    "        abstract = getInfoFromFile(file_location)\n",
    "        file_content = open(file_location,\"r\").readlines()\n",
    "\n",
    "        #traversing all lines in each file (each line has a json with information about frames)\n",
    "        for index, line in enumerate(file_content):\n",
    "            if(index > 0):\n",
    "                jsonline = json.loads(line)\n",
    "\n",
    "                #verifying each item    \n",
    "                for frame in jsonline.get('frames'):\n",
    "\n",
    "                    if(item_type == 'frame'):\n",
    "                        frame_name = frame.get('target').get('name').lower()\n",
    "                        if(frame_name[len(frame_name)-1] == 's' or frame_name[len(frame_name)-4:len(frame_name)-1] == 'ies'):\n",
    "                            frame_name =inflection.singularize(frame_name)\n",
    "                        if params:\n",
    "                            if(abstract.get(category_type) == category):\n",
    "                                if(itens_counted.get(frame_name) != None):\n",
    "                                    itens_counted[frame_name] += 1 \n",
    "                                else:\n",
    "                                    itens_counted[frame_name] = 1                                \n",
    "                        else:\n",
    "                                if(itens_counted.get(frame_name) != None):\n",
    "                                    itens_counted[frame_name] += 1 \n",
    "                                else:\n",
    "                                    itens_counted[frame_name] = 1                     \n",
    "\n",
    "                    if(item_type == 'lexical_unit'):\n",
    "                        for span in frame.get('target').get('spans'):\n",
    "                            lexical_unit = span.get('text').lower() \n",
    "                            if(lexical_unit[len(lexical_unit)-1] == 's' or lexical_unit[len(lexical_unit)-4:len(lexical_unit)-1] == 'ies'):\n",
    "                                lexical_unit =inflection.singularize(lexical_unit) \n",
    "                            if params:\n",
    "                                if(abstract.get(category_type) == category):   \n",
    "                                    #se ja existir o index, soma\n",
    "                                    if(itens_counted.get(lexical_unit) != None):\n",
    "                                        itens_counted[lexical_unit] += 1 \n",
    "                                    #senao cria o index sem modificacoes\n",
    "                                    else:\n",
    "                                        itens_counted[lexical_unit] = 1     \n",
    "                            else:\n",
    "                                if(itens_counted.get(lexical_unit) != None):\n",
    "                                    itens_counted[lexical_unit] += 1 \n",
    "                                else:\n",
    "                                    itens_counted[lexical_unit] = 1 \n",
    "    return itens_counted\n",
    "\n",
    "# files = the files where the function should search for itens\n",
    "# params = (optional) --- do not pass when considering all file\n",
    "#       [0] the categories of abstract that the function will search in --- do not pass when considering all files\n",
    "#       [1] the category type (field, discipline, journal)  \n",
    "\n",
    "def itensPerCategory(files, item_type, *params):\n",
    "    path = \"../semafor_output\" #files path\n",
    "    mensagem = \"Dados de: \\n\"  #log\n",
    "    result = dict()            #output \n",
    "    \n",
    "    #considering all files\n",
    "    if not params: \n",
    "        print(\"Searching for \"+item_type+\"s in all files\")\n",
    "        all_result = dict()\n",
    "        #all_result = first30(sortDict(countItens(item_type,all_result)))\n",
    "        all_result = sortDict(countItens(item_type,all_result))\n",
    "        qtd = len(all_result)\n",
    "        cutstart = 0\n",
    "        while(cutstart < qtd):\n",
    "            generateChart(cutdict(all_result,cutstart,30), 'all', 'all ' + str(cutstart)+\"_\"+str(cutstart+30), item_type)\n",
    "            cutstart = cutstart+30\n",
    "        \n",
    "        return all_result\n",
    "    else:\n",
    "        categories = params[0]\n",
    "        category_type = params[1] \n",
    "        print(\"Searching for \"+item_type+\"s per \"+category_type+\"s\")\n",
    "        all_categories_result = dict()\n",
    "        for category in categories:\n",
    "            category_result = dict()\n",
    "            #category_result = first30(sortDict(countItens(item_type, category_result, category, category_type)))\n",
    "            category_result = sortDict(countItens(item_type, category_result, category, category_type))\n",
    "            if(category_result):\n",
    "                label = 'Lexical Units' if (item_type == 'lexical_unit') else 'Frames'\n",
    "                qtd = len(category_result)\n",
    "                cutstart = 0\n",
    "                while(cutstart < qtd):\n",
    "                    generateChart(cutdict(category_result,cutstart,30), category_type, category+\" (\"+label+\") \"+str(cutstart)+\"_\"+str(cutstart+30), item_type)\n",
    "                    cutstart = cutstart+30\n",
    "                all_categories_result[category] = category_result\n",
    "            else:\n",
    "                print(\"Nothing found for \"+category_type+\": \"+category)\n",
    "        return all_categories_result\n",
    "\n",
    "#function that generates XLS files\n",
    "def generateExcel(fpcat, opt, optdir):\n",
    "    if(opt != \"all\"):\n",
    "        for v in fpcat:\n",
    "            vDict = fpcat.get(v)\n",
    "            vDF = pd.DataFrame({optdir+'s':list(vDict.keys()),'occurrence':list(vDict.values())})\n",
    "            vDF.to_excel(\"export/\"+opt+\"/\"+optdir+\"/\"+v+\".xlsx\", index=False)\n",
    "    else: \n",
    "        fpcat = pd.DataFrame({optdir+'s':list(fpcat.keys()),'occurrence':list(fpcat.values())})\n",
    "        fpcat.to_excel(\"export/\"+opt+\"/\"+optdir+\"/All_Abstracts.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for frames in all files\n",
      "Searching for frames per fields\n",
      "Searching for frames per disciplines\n",
      "Searching for frames per journals\n",
      "Nothing found for journal: ADVANCES IN AGRONOMY, VOL 136\n",
      "Nothing found for journal: QUALITY & SAFETY\n",
      "Searching for lexical_units in all files\n",
      "Searching for lexical_units per fields\n",
      "Searching for lexical_units per disciplines\n",
      "Searching for lexical_units per journals\n",
      "Nothing found for journal: ADVANCES IN AGRONOMY, VOL 136\n",
      "Nothing found for journal: QUALITY & SAFETY\n"
     ]
    }
   ],
   "source": [
    "# Selecting json files\n",
    "path = \"../semafor_output\"\n",
    "files = os.listdir(path)\n",
    "cleanDirectory(files)\n",
    "\n",
    "# Sorting files, disciplines and journals alphabetically\n",
    "list_of_fields = list(getAllFields())\n",
    "list_of_fields.sort()\n",
    "\n",
    "list_of_disciplines = list(getAllDisciplines())\n",
    "list_of_disciplines.sort()\n",
    "\n",
    "list_of_journals = list(getAllJournals())\n",
    "list_of_journals.sort()\n",
    "\n",
    "# Creating the directories needed\n",
    "createDirectories()\n",
    "\n",
    "all_frames = itensPerCategory(files,'frame')\n",
    "frames_per_field = itensPerCategory(files, 'frame', list_of_fields, 'field')\n",
    "frames_per_discipline = itensPerCategory(files,'frame', list_of_disciplines, 'discipline')\n",
    "frames_per_journal = itensPerCategory(files,'frame', list_of_journals, 'journal')\n",
    "\n",
    "all_lexicalunits = itensPerCategory(files,'lexical_unit')\n",
    "lexicalunits_per_field = itensPerCategory(files, 'lexical_unit', list_of_fields, 'field')\n",
    "lexicalunits_per_discipline = itensPerCategory(files,'lexical_unit', list_of_disciplines, 'discipline')\n",
    "lexicalunits_journal = itensPerCategory(files,'lexical_unit', list_of_journals, 'journal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xls files for all abstracts, by field, by discipline and by journal (frames)\n",
    "generateExcel(all_frames,\"all\",'frame')\n",
    "generateExcel(frames_per_field,\"field\",'frame')\n",
    "generateExcel(frames_per_discipline,\"discipline\",'frame')\n",
    "generateExcel(frames_per_journal,\"journal\",'frame')\n",
    "\n",
    "#xls files for all abstracts, by field, by discipline and by journal (lexical units)\n",
    "generateExcel(all_lexicalunits,\"all\",'lexical_unit')\n",
    "generateExcel(lexicalunits_per_field,\"field\",'lexical_unit')\n",
    "generateExcel(lexicalunits_per_discipline,\"discipline\",'lexical_unit')\n",
    "generateExcel(lexicalunits_journal,\"journal\",'lexical_unit')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generatePieChart(data,dir_filename):\n",
    "    # Pie chart\n",
    "    labels = data.keys()\n",
    "    sizes = data.values()\n",
    "    \n",
    "    # only \"explode\" the 2nd slice (i.e. 'Hogs')\n",
    "    explode = (0, 0.1, 0, 0)  \n",
    "    fig1, ax1 = plt.subplots()\n",
    "    ax1.pie(sizes, labels=labels, autopct='%1.1f%%', textprops={'size': 'smaller'}, pctdistance=1.1,\n",
    "             labeldistance=1.3, startangle=90)\n",
    "    \n",
    "    # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "    ax1.axis('equal')  \n",
    "    plt.tight_layout()\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(10,10)  \n",
    "    plt.savefig(dir_filename+'.png', bbox_inches='tight', dpi=300)\n",
    "    #plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_total = sum(all_frames.values())\n",
    "f_field_total = dict()\n",
    "f_discipline_total = dict()\n",
    "f_journal_total = dict()\n",
    "\n",
    "lu_total = sum(all_lexicalunits.values())\n",
    "lu_field_total = dict()\n",
    "lu_discipline_total = dict()\n",
    "lu_journal_total = dict()\n",
    "\n",
    "for field in frames_per_field:\n",
    "    f_field_total[field] = sum(frames_per_field.get(field).values()) \n",
    "for discipline in frames_per_discipline:\n",
    "    f_discipline_total[discipline] = sum(frames_per_discipline.get(discipline).values())\n",
    "for journal in frames_per_journal:\n",
    "    f_journal_total[journal] = sum(frames_per_journal.get(journal).values())\n",
    "\n",
    "for field in lexicalunits_per_field:\n",
    "    lu_field_total[field] = sum(lexicalunits_per_field.get(field).values())\n",
    "for discipline in lexicalunits_per_discipline:\n",
    "    lu_discipline_total[discipline] = sum(lexicalunits_per_discipline.get(discipline).values())\n",
    "for journal in lexicalunits_journal:\n",
    "    lu_journal_total[journal] = sum(lexicalunits_journal.get(journal).values())\n",
    "    \n",
    "f_percen = dict()\n",
    "f_field_percen = dict()\n",
    "f_discipline_percen = dict()\n",
    "f_journal_percen = dict()\n",
    "\n",
    "lu_percen = dict()\n",
    "lu_field_percen = dict()\n",
    "lu_discipline_percen = dict()\n",
    "lu_journal_percen = dict()\n",
    "\n",
    "for frame in all_frames:\n",
    "        f_percen[frame] = (all_frames.get(frame)/f_total)*100        \n",
    "f_percen = sortDict(f_percen) \n",
    "\n",
    "for field in frames_per_field:\n",
    "    f_field_percen[field] = dict()\n",
    "    for frame in frames_per_field.get(field):\n",
    "        f_field_percen[field][frame] = (frames_per_field.get(field).get(frame)/f_field_total.get(field))*100\n",
    "\n",
    "for discipline in frames_per_discipline:\n",
    "    f_discipline_percen[discipline] = dict()\n",
    "    for frame in frames_per_discipline.get(discipline):\n",
    "        f_discipline_percen[discipline][frame] = (frames_per_discipline.get(discipline).get(frame)/f_discipline_total.get(discipline))*100\n",
    "\n",
    "for journal in frames_per_journal:\n",
    "    f_journal_percen[journal] = dict()\n",
    "    for frame in frames_per_journal.get(journal):\n",
    "        f_journal_percen[journal][frame] = (frames_per_journal.get(journal).get(frame)/f_journal_total.get(journal))*100\n",
    "\n",
    "for lexical_unit in all_lexicalunits:\n",
    "        lu_percen[lexical_unit] = (all_lexicalunits.get(lexical_unit)/lu_total)*100        \n",
    "lu_percen = sortDict(lu_percen) \n",
    "\n",
    "for field in lexicalunits_per_field:\n",
    "    lu_field_percen[field] = dict()\n",
    "    for lexical_unit in lexicalunits_per_field.get(field):\n",
    "        lu_field_percen[field][lexical_unit] = (lexicalunits_per_field.get(field).get(lexical_unit)/lu_field_total.get(field))*100\n",
    "        \n",
    "for discipline in lexicalunits_per_discipline:\n",
    "    lu_discipline_percen[discipline] = dict()\n",
    "    for lexical_unit in lexicalunits_per_discipline.get(discipline):\n",
    "        lu_discipline_percen[discipline][lexical_unit] = (lexicalunits_per_discipline.get(discipline).get(lexical_unit)/lu_discipline_total.get(discipline))*100\n",
    "\n",
    "for journal in lexicalunits_journal:\n",
    "    lu_journal_percen[journal] = dict()\n",
    "    for lexical_unit in lexicalunits_journal.get(journal):\n",
    "        lu_journal_percen[journal][lexical_unit] = (lexicalunits_journal.get(journal).get(lexical_unit)/lu_journal_total.get(journal))*100\n",
    "\n",
    "def less_than_one(dict_percen):\n",
    "    less_than_one_percent = 0;\n",
    "    for percen in dict_percen.copy():\n",
    "        if(dict_percen.get(percen) < 1):\n",
    "            less_than_one_percent += dict_percen.get(percen)\n",
    "            dict_percen.pop(percen)\n",
    "    dict_percen['Less than one percent'] = less_than_one_percent\n",
    "    return dict_percen\n",
    "\n",
    "def more_than_one(dict_percen):\n",
    "    for percen in dict_percen.copy():\n",
    "        if(dict_percen.get(percen) < 1):\n",
    "            dict_percen.pop(percen)\n",
    "    return dict_percen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = less_than_one(f_percen) \n",
    "generatePieChart(data,'export/'+'all'+'/'+'frame/'+'all'+'_lto')\n",
    "data = more_than_one(f_percen) \n",
    "generatePieChart(data,'export/'+'all'+'/'+'frame/'+'all'+'_mto')\n",
    "\n",
    "for field in f_field_percen:\n",
    "    data = less_than_one(sortDict(f_field_percen.get(field)))\n",
    "    generatePieChart(data,'export/'+'field'+'/'+'frame/'+field+'_lto')\n",
    "    data = more_than_one(sortDict(f_field_percen.get(field)))\n",
    "    generatePieChart(data,'export/'+'field'+'/'+'frame/'+field+'_mto')\n",
    "    \n",
    "for discipline in f_discipline_percen:\n",
    "    data = less_than_one(sortDict(f_discipline_percen.get(discipline)))\n",
    "    generatePieChart(data,'export/'+'discipline'+'/'+'frame/'+discipline+'_lto')\n",
    "    data = more_than_one(sortDict(f_discipline_percen.get(discipline)))\n",
    "    generatePieChart(data,'export/'+'discipline'+'/'+'frame/'+discipline+'_mto')\n",
    "        \n",
    "for journal in f_journal_percen:\n",
    "    data = less_than_one(sortDict(f_journal_percen.get(journal)))\n",
    "    generatePieChart(data,'export/'+'journal'+'/'+'frame/'+journal+'_lto')\n",
    "    data = more_than_one(sortDict(f_journal_percen.get(journal)))\n",
    "    generatePieChart(data,'export/'+'journal'+'/'+'frame/'+journal+'_mto') \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = less_than_one(lu_percen) \n",
    "generatePieChart(data,'export/'+'all'+'/'+'lexical_unit/'+'all'+'_lto')\n",
    "data = more_than_one(lu_percen) \n",
    "generatePieChart(data,'export/'+'all'+'/'+'lexical_unit/'+'all'+'_mto')\n",
    "\n",
    "for field in lu_field_percen:\n",
    "    data = less_than_one(sortDict(lu_field_percen.get(field)))\n",
    "    generatePieChart(data,'export/'+'field'+'/'+'lexical_unit/'+field+'_lto')\n",
    "    data = more_than_one(sortDict(lu_field_percen.get(field)))\n",
    "    generatePieChart(data,'export/'+'field'+'/'+'lexical_unit/'+field+'_mto')\n",
    "    \n",
    "for discipline in lu_discipline_percen:\n",
    "    data = less_than_one(sortDict(lu_discipline_percen.get(discipline)))\n",
    "    generatePieChart(data,'export/'+'discipline'+'/'+'lexical_unit/'+discipline+'_lto')\n",
    "    data = more_than_one(sortDict(lu_discipline_percen.get(discipline)))\n",
    "    generatePieChart(data,'export/'+'discipline'+'/'+'lexical_unit/'+discipline+'_mto')\n",
    "        \n",
    "for journal in lu_journal_percen:\n",
    "    data = less_than_one(sortDict(lu_journal_percen.get(journal)))\n",
    "    generatePieChart(data,'export/'+'journal'+'/'+'lexical_unit/'+journal+'_lto')\n",
    "    data = more_than_one(sortDict(lu_journal_percen.get(journal)))\n",
    "    generatePieChart(data,'export/'+'journal'+'/'+'lexical_unit/'+journal+'_mto') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = lu_percen\n",
    "for percen in data.copy():\n",
    "    if(data.get(percen) < 0.5):\n",
    "        data.pop(percen) \n",
    "\n",
    "generatePieChart(data,'export/'+'all'+'/'+'lexical_unit/'+'all'+'_mthalfpercent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('notebook_env.db') #save\n",
    "dill.load_session('notebook_env.db') #load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countLexicalFrame(source, categ_type):\n",
    "    counted = dict()\n",
    "    for categ in source:\n",
    "        categ_data = dict()\n",
    "        if(categ_type == 'all'):\n",
    "            categ_data = source\n",
    "        else:\n",
    "            categ_data = source.get(categ) \n",
    "        for frameindex in categ_data:\n",
    "            #traversing all files\n",
    "            for file_name in files:            \n",
    "                file_location = path+'/'+file_name\n",
    "                abstract = getInfoFromFile(file_location) \n",
    "                if(abstract.get(categ_type) == categ or categ_type == 'all'): \n",
    "                    file_content = open(file_location,\"r\").readlines()\n",
    "\n",
    "                    #traversing all lines in each file (each line has a json with information about frames)\n",
    "                    for index, line in enumerate(file_content):\n",
    "                        if(index > 0):\n",
    "                            jsonline = json.loads(line)  \n",
    "\n",
    "                            #verifying each item    \n",
    "                            for frame in jsonline.get('frames'):                            \n",
    "                                frame_name = frame.get('target').get('name').lower()  \n",
    "                                if(frame_name == frameindex): \n",
    "                                    for span in frame.get('target').get('spans'):\n",
    "                                        lexical_unit = span.get('text').lower()  \n",
    "                                        if(counted.get(categ) == None):  \n",
    "                                            counted[categ] = dict()  \n",
    "                                        if(counted.get(categ).get(frame_name) == None):   \n",
    "                                            counted[categ][frame_name] = dict()\n",
    "                                        if(counted.get(categ).get(frame_name).get(lexical_unit) == None):\n",
    "                                            counted[categ][frame_name][lexical_unit] = 1\n",
    "                                        else:\n",
    "                                            counted[categ][frame_name][lexical_unit] += 1\n",
    "                                        #print(counted)\n",
    "    return counted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lu_per_frame_all = dict()\n",
    "lu_per_frame_field = dict()\n",
    "lu_per_frame_discipline = dict()\n",
    "lu_per_frame_journal = dict()\n",
    "                                    \n",
    "lu_per_frame_all = countLexicalFrame(cutdict(all_frames,0,30),'all')\n",
    "lu_per_frame_field = countLexicalFrame(cutdict(frames_per_field,0,30),'field')\n",
    "lu_per_frame_discipline = countLexicalFrame(cutdict(frames_per_discipline,0,30),'discipline')\n",
    "lu_per_frame_journal = countLexicalFrame(cutdict(frames_per_journal,0,30),'journal')\n",
    "                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dill.dump_session('notebook_env.db') #save\n",
    "dill.load_session('notebook_env.db') #load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for field in lu_per_frame_field: \n",
    "    for frame in lu_per_frame_field.get(field):  \n",
    "        generateChart(sortDict(lu_per_frame_field.get(field).get(frame)), 'field', 'LUs by Frame ('+field+' - '+frame+')', 'lexical_unit')   \n",
    "    \n",
    "for discipline in lu_per_frame_discipline:\n",
    "    for frame in lu_per_frame_discipline.get(discipline): \n",
    "        generateChart(sortDict(lu_per_frame_discipline.get(discipline).get(frame)), 'discipline', 'LUs by Frame ('+discipline+' - '+frame+')', 'lexical_unit')   \n",
    "    \n",
    "for journal in lu_per_frame_journal:\n",
    "    for frame in lu_per_frame_journal.get(journal): \n",
    "        generateChart(sortDict(lu_per_frame_journal.get(journal).get(frame)), 'journal', 'LUs by Frame ('+journal+' - '+frame+')', 'lexical_unit')   \n",
    "    \n",
    "for frame in lu_per_frame_all:\n",
    "    generateChart(sortDict(lu_per_frame_all.get(frame)), 'all', 'LUs by Frame ('+'all'+' - '+frame+')', 'lexical_unit')   \n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_percen = dict();\n",
    "for frame in all_frames:\n",
    "        f_percen[frame] = (all_frames.get(frame)/f_total)*100        \n",
    "f_percen = sortDict(f_percen) \n",
    "#print(f_percen)\n",
    "\n",
    "lu_percen = dict();\n",
    "for lexical_unit in all_lexicalunits:\n",
    "        lu_percen[lexical_unit] = (all_lexicalunits.get(lexical_unit)/lu_total)*100        \n",
    "lu_percen = sortDict(lu_percen) \n",
    "#print(lu_percen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def less_than_one(dict_percen):\n",
    "    less_than_one_percent = 0;\n",
    "    for percen in dict_percen.copy():\n",
    "        if(dict_percen.get(percen) < 1):\n",
    "            less_than_one_percent += dict_percen.get(percen)\n",
    "            dict_percen.pop(percen)\n",
    "    dict_percen['Less than one percent'] = less_than_one_percent\n",
    "    return dict_percen\n",
    "\n",
    "def more_than_one(dict_percen):\n",
    "    for percen in dict_percen.copy():\n",
    "        if(dict_percen.get(percen) < 1):\n",
    "            dict_percen.pop(percen)\n",
    "    return dict_percen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'quantity': 2.596561386710224, 'scrutiny': 2.2445329991410508, 'mean': 2.100905417012828, 'cardinal_number': 2.0037455820437358, 'gizmo': 1.7038173958348002, 'statement': 1.6615739893264994, 'using': 1.4531731838855486, 'purpose': 1.4010729825253108, 'causation': 1.3982567554247574, 'temporal_collocation': 1.3208105101595393, 'aggregate': 1.2856076714026219, 'text': 1.2152019938887872, 'capability': 1.1574693383274426, 'likelihood': 1.1462044299252292, 'objective_influence': 1.096920455665545, 'increment': 1.0856555472633314, 'similarity': 1.0490445949561373, 'building': 1.02651477815171, 'Less than one percent': 73.05293098835548}\n",
      "{'quantity': 2.596561386710224, 'scrutiny': 2.2445329991410508, 'mean': 2.100905417012828, 'cardinal_number': 2.0037455820437358, 'gizmo': 1.7038173958348002, 'statement': 1.6615739893264994, 'using': 1.4531731838855486, 'purpose': 1.4010729825253108, 'causation': 1.3982567554247574, 'temporal_collocation': 1.3208105101595393, 'aggregate': 1.2856076714026219, 'text': 1.2152019938887872, 'capability': 1.1574693383274426, 'likelihood': 1.1462044299252292, 'objective_influence': 1.096920455665545, 'increment': 1.0856555472633314, 'similarity': 1.0490445949561373, 'building': 1.02651477815171}\n"
     ]
    }
   ],
   "source": [
    "lu_lto = less_than_one(lu_percen.copy()) \n",
    "lu_mto = more_than_one(lu_percen.copy())\n",
    "fr_lto = less_than_one(f_percen.copy()) \n",
    "fr_mto = more_than_one(f_percen.copy())\n",
    "print(fr_lto)\n",
    "print(fr_mto)\n",
    "generatePieChart(fr_mto,'export/'+'all'+'/'+'frame/'+'all'+'_mto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
